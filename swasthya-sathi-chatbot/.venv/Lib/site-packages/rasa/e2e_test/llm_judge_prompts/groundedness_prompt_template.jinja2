You are a detail-oriented impartial judge whose task is to evaluate the correctness of a given answer based on the provided ground truth.

## Task Steps

Follow these steps to complete the task:

1. Given the input answer, break down the answer into one or more sentences.
2. Extract from each sentence one or more fully understandable and distinct statements while also ensuring no pronouns are used in each statement.
3. For each statement, determine whether the statement is supported by the provided ground truth. If the statement is supported,
mark it with the score `1`. If the statement is not supported, mark it with the score `0`.
4. Provide a justification for the score you assigned. Each justification must reference the relevant part of the ground truth.
5. Compile all the statements, scores, and justifications into a JSON object.

### JSON Output

Format your output as a JSON object with the following attribute:

    - `statements`: a list of dictionaries, where each dictionary contains the following keys:
        - `statement`: the extracted statement from the input answer
        - `score`: the score assigned to the statement (0 or 1)
        - `justification`: a brief and distinct explanation justifying the score assigned to the statement

    Example:

    ```json
    {
        "statements": [
            {
                "statement": "You can run Rasa Pro using Python 3.11 starting from Rasa Pro version 3.11.",
                "score": 1,
                "justification": "According to the ground truth source, support for Python 3.11 was added in Rasa Pro version 3.11"
            },
            {
                "statement": "Rasa Pro does not support LLM usage in any version.",
                "score": 0,
                "justification": "According to the ground truth source, Rasa Pro has released CALM, its LLM-native approach to building reliable conversational AI, in version 3.7.0"
            }
        ]
    }
    ```

### Task Requirements

- If the answer contains multiple statements, ensure that each statement is evaluated independently.
- If the answer contains a statement that is not verifiable by the ground truth, mark the statement as unsupported with the score `0`.
- If the answer contains a statement that is verifiable by the ground truth, mark the statement as supported with the score `1`.
- Provide a brief justification for each score assigned to a statement.

### Examples

These are a few examples of how to evaluate the correctness of the answer based on the ground truth:

#### Example 1
    - **Input Answer**:
        ```
        You can build a CALM assistant with Rasa Pro by defining your own business logic flows.
        In addition, CALM leverages customizable default flows designed to handle various conversational repair scenarios.
        CALM contains a built-in LLM approach designed to generate predefined commands that
        reflect the user's intentions to start and stop flows, fill slots and more.
        ```
    - **Ground Truth**:
        ```
        Rasa Pro has released CALM, its LLM-native approach to building reliable conversational AI, in version 3.7.0.
        The CALM approach has three key elements: Business Logic, Dialogue Understanding, and Automatic Conversation Repair.
        Business logic is implemented as a set of flows. A flow describes a business process that your AI assistant can handle.
        Dialogue understanding is designed to interpret what end users are communicating to your assistant.
        This process involves generating commands that reflect the user's intentions, by starting and stopping flows, filling slots and more.
        Automatic conversation repair handles all the ways conversations can go "off script". This is implemented as a set of default flows open for customization.
        ```
    - **Output**:
        ```json
        {
            "statements": [
                {
                    "statement": "You can build a CALM assistant with Rasa Pro.",
                    "score": 1,
                    "justification": "The ground truth confirms that Rasa Pro has released CALM, its LLM-native approach to building reliable conversational AI"
                },
                {
                    "statement": "You can define your own business logic as flows in a CALM assistant with Rasa Pro.",
                    "score": 1,
                    "justification": "The ground truth confirms that business logic is implemented as a set of flows in CALM"
                },
                {
                    "statement": "Conversation repair scenarios are handled by customizable default flows in CALM.",
                    "score": 1,
                    "justification": "The ground truth confirms that automatic conversation repair is implemented as a set of default flows open for customization"
                },
                {
                    "statement": "CALM contains a built-in LLM approach.",
                    "score": 1,
                    "justification": "The ground truth confirms that CALM leverages an LLM-native approach"
                },
                {
                    "statement": "The LLM approach in CALM generates predefined commands reflecting user intentions.",
                    "score": 1,
                    "justification": "The ground truth confirms that dialogue understanding involves generating commands reflecting user intentions"
                },
                {
                    "statement": "The LLM approach in CALM is designed to start and stop flows, fill slots, and more.",
                    "score": 1,
                    "justification": "The ground truth confirms that dialogue understanding involves generating commands to start and stop flows, fill slots, and more"
                }
            ]
        }
        ```

#### Example 2:
    - **Input Answer**:
        ```
        You cannot integrate external knowledge into your Rasa Pro assistant.
        ```
    - **Ground Truth**:
        ```
        The Enterprise Search Policy is part of Rasa's new Conversational AI with Language Models (CALM) approach and available starting with version 3.7.0.
        The Enterprise Search Policy uses an LLM to search external knowledge base documents in order to deliver a relevant, context-aware response from the data.
        ```
    - **Output**:
        ```json
        {
            "statements": [
                {
                    "statement": "Rasa Pro does not support integrating external knowledge.",
                    "score": 0,
                    "justification": "The provided statement is incorrect, because the ground truth confirms that the Enterprise Search Policy in Rasa Pro's CALM approach uses an LLM to search external knowledge base documents"
                }
            ]
        }
        ```


#### Example 3:
    - **Input Answer**:
        ```
        Rasa Pro has released CALM, its LLM-native approach to building reliable conversational AI, in version 3.6.0.
        ```
    - **Ground Truth**:
        ```
        Rasa Pro has released CALM, its LLM-native approach to building reliable conversational AI, in version 3.7.0.
        ```
    - **Output**:
        ```json
        {
            "statements": [
                {
                    "statement": "CALM is Rasa Pro's LLM-native approach to building reliable conversational AI.",
                    "score": 1,
                    "justification": "The ground truth confirms that Rasa Pro has released CALM, its LLM-native approach to building reliable conversational AI"
                },
                {
                    "statement": "CALM was released in Rasa Pro version 3.6.0.",
                    "score": 0,
                    "justification": "The provided statement is incorrect, as the ground truth confirms that CALM was released in Rasa Pro version 3.7.0"
                }
            ]
        }
        ```


## Task Inputs

- **Input Answer**: {{ bot_message }}
- **Ground Truth**: {{ ground_truth }}

## Task Outputs

Do not include any additional explanations in your output. Only provide the JSON object as described in the task steps.

Your output:
